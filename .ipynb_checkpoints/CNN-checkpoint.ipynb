{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rCpZdGFnjXuQ"
   },
   "outputs": [],
   "source": [
    "#import cnn api modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import pdb\n",
    "\n",
    "#for training\n",
    "import tqdm\n",
    "import torch.optim as optim\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = os.getcwd()+'/data/train_img'\n",
    "CATAGORIES = [\"/car\"]\n",
    "path = DATADIR + CATAGORIES[0]\n",
    "for img in os.listdir(path):\n",
    "    if \".jpg\" in img:\n",
    "        #m = plt.imread(path + '/' + img) \n",
    "        #plt.imshow(m,cmap='gray',interpolation='none')\n",
    "        #plt.show()\n",
    "        \n",
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kO72OtE5jn0Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/TaoYao/Google Drive/UW_Master/MSEE/2019_fall/EE_596/EE596_final/eep596_machine_vision_final/data/train_img/car\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: /Users/TaoYao/Google Drive/UW_Master/MSEE/2019_fall/EE_596/EE596_final/eep596_machine_vision_final/data/train_img/car\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif,.tiff,.webp",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d2e46f4efa6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Setup the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m ds = torchvision.datasets.ImageFolder(path,\n\u001b[0;32m---> 15\u001b[0;31m                                      transform=trans_)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Setup the dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda3/lib/python3.7/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n\u001b[0;32m---> 97\u001b[0;31m                                 \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: /Users/TaoYao/Google Drive/UW_Master/MSEE/2019_fall/EE_596/EE596_final/eep596_machine_vision_final/data/train_img/car\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif,.tiff,.webp"
     ]
    }
   ],
   "source": [
    "#init data loader\n",
    "\n",
    "# transform to do random affine and cast image to PyTorch tensor\n",
    "trans_ = torchvision.transforms.Compose(\n",
    "    [\n",
    "     # torchvision.transforms.RandomAffine(10),\n",
    "     torchvision.transforms.ToTensor()] #transform from height*width*channel to ch*h*w in order to fit tourch tensor format\n",
    ")\n",
    "\n",
    "# Setup the dataset\n",
    "ds = torchvision.datasets.ImageFolder(path,\n",
    "                                     transform=trans_)\n",
    "\n",
    "# Setup the dataloader\n",
    "loader = torch.utils.data.DataLoader(ds, \n",
    "                                     batch_size=16, #batch is how many imgs/samples load per loop\n",
    "                                     shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t8RCXkdnkMyr"
   },
   "outputs": [],
   "source": [
    "# [16, 3, 30, 30] = [batch size, channels, width, height]\n",
    "for x, y in loader:\n",
    "    print(x.shape) #the img \n",
    "    print(y.shape) #tensor dim; here'll be 16 instead of 16*5\n",
    "    print(y) #tensor\n",
    "    break\n",
    "\n",
    "# vis\n",
    "for i in range(16):\n",
    "    plt.imshow(np.transpose(x[i,:], (1,2,0))) # 30 x 30 x 3\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lu3Ou1Brkmj2"
   },
   "outputs": [],
   "source": [
    "#the cnn class which inherit from torch.nn.Module class\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self): #constructor\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        # define the layers\n",
    "        # kernel size = 3 means (3,3) kernel\n",
    "        # rgb -> 3 -> in channel\n",
    "        # number of feature maps = 16\n",
    "        # number of filters = 3 x 16\n",
    "        self.l1 = nn.Conv2d(kernel_size=3, in_channels=3, out_channels=16) #1st convolve layer\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2) #down sampling layer\n",
    "        # MaxPool2d, AvgPool2d. \n",
    "        # The first 2 = 2x2 kernel size, \n",
    "        # The second 2 means the stride=2\n",
    "        \n",
    "        self.l2 = nn.Conv2d(kernel_size=3, in_channels=16, out_channels=32) #2nd convolve layer\n",
    "        \n",
    "        # FC layer (fully-connected or linear layer)\n",
    "        self.fc1 = nn.Linear(32 * 6 * 6, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # define the data flow through the deep learning layers\n",
    "        #(1st conv->pool layer)\n",
    "        x = self.pool(F.relu(self.l1(x))) # 16x16 x 14 x 14 \n",
    "        #(2nd conv->pool layer)\n",
    "        x = self.pool(F.relu(self.l2(x))) # 16x32x6x6 \n",
    "        # print(x.shape)\n",
    "        #flatten layer, set -1 coz last batch might not be full\n",
    "        x = x.reshape(-1, 32*6*6) # [16 x 1152]# CRUCIAL: \n",
    "        # print(x.shape)\n",
    "        #FC layer\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yyVB4Se1lt5_"
   },
   "outputs": [],
   "source": [
    "m = CNN() #init a brand-new untrained cnn\n",
    "pred = m(x) #prediction\n",
    "print(pred.shape) #vector will be encoded to 16*5\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DMggNVt9nPVD"
   },
   "outputs": [],
   "source": [
    "#now it's time for training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epoches = 50\n",
    "\n",
    "#our training loop\n",
    "for epoch_id in range(num_epoches):\n",
    "    optimizer = optim.SGD(m.parameters(), lr=0.01 * 0.95 ** epoch_id)\n",
    "    for x, y in tqdm.tqdm(loader):\n",
    "        optimizer.zero_grad() # clear (reset) the gradient for the optimizer\n",
    "        pred = m(x)\n",
    "        loss = criterion(pred, y)\n",
    "        loss.backward() # calculating the gradient\n",
    "        optimizer.step() # backpropagation: optimize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_qYKALnYndzi"
   },
   "outputs": [],
   "source": [
    "#after training, test phase test the result include accuracy\n",
    "# Setup the dataset\n",
    "test_ds = torchvision.datasets.ImageFolder(\"test_img/\",\n",
    "                                     transform=trans_)\n",
    "\n",
    "# Setup the dataloader\n",
    "testloader = torch.utils.data.DataLoader(test_ds, \n",
    "                                     batch_size=16, \n",
    "                                     shuffle=True)\n",
    "\n",
    "all_gt = []\n",
    "all_pred = []\n",
    "\n",
    "for x, y in tqdm.tqdm(loader):\n",
    "    optimizer.zero_grad() # clear (reset) the gradient for the optimizer\n",
    "    all_gt += list(y.numpy().reshape(-1))\n",
    "    pred = torch.argmax(m(x), dim=1)\n",
    "    all_pred += list(pred.numpy().reshape(-1))\n",
    "\n",
    "print(all_gt)\n",
    "print(all_pred)\n",
    "acc = np.sum(np.array(all_gt) == np.array(all_pred)) / len(all_gt)\n",
    "print(\"Accuracy is:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XkisEwGpntAy"
   },
   "outputs": [],
   "source": [
    "#Dilation and Depth-wise Conv\n",
    "standard_conv = nn.Conv2d(kernel_size=3, in_channels=16, out_channels=16, dilation=1, groups=1)\n",
    "#the only difference vs sd_conv is dil_conv has dilation=2\n",
    "dilated_conv = nn.Conv2d(kernel_size=3, in_channels=16, out_channels=16, dilation=2, groups=1)\n",
    "#the only difference vs sd_conv is dep_conv has groups=16\n",
    "depth_conv = nn.Conv2d(kernel_size=3, in_channels=16, out_channels=16, dilation=1, groups=16)\n",
    "print(sum([p.numel() for p in standard_conv.parameters()]))\n",
    "print(sum([p.numel() for p in dilated_conv.parameters()]))\n",
    "print(sum([p.numel() for p in depth_conv.parameters()]))\n",
    "print(standard_conv.weight.shape)\n",
    "print(dilated_conv.weight.shape)\n",
    "print(depth_conv.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fW2yco28n_3A"
   },
   "source": [
    "To do:\n",
    "11/22/2019 (Fri):\n",
    "1. find and organize data folder, load and visualize out all imgs\n",
    "2. feed imgs to 2 layer cnn and test result\n",
    "3. Learnt unknown concepts from Questions\n",
    "\n",
    "11/23-11/24 (Sat,Sun):\n",
    "1. Figure out a way to return obj type and pass it to moving obj\n",
    "2. add more layers (deeper network), higher resolution img (300*300)\n",
    "3. Use Dilation and depth conv\n",
    "\n",
    "Questions:\n",
    "  1. What is tensor?\n",
    "  2. in class CNN, how to determin what kind of conv kernels each layer is using?\n",
    "  3. epoch, criterion, optim.SGD, cross-entropy loss?\n",
    "  4. Dilation and depth conv?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
